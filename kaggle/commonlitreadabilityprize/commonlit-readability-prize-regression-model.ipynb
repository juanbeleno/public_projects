{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict readability score using text\n\nWe verify that GPU is available for training using pytorch.\n\n## Ideas\n\n* Cross validation K-Folds\n* Ensembles of 6 models","metadata":{}},{"cell_type":"code","source":"!nvidia-smi\n# Verify CUDA version is 11.0+\n!nvcc -V\n\n# Verify pytorch is using the GPU\nimport torch\nprint(f'Torch: {torch.__version__}')\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:44:23.281932Z","iopub.execute_input":"2021-05-22T21:44:23.282242Z","iopub.status.idle":"2021-05-22T21:44:25.807214Z","shell.execute_reply.started":"2021-05-22T21:44:23.28217Z","shell.execute_reply":"2021-05-22T21:44:25.806298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\nFirst, we need to load all the datasets that we are going to use to train the model and generate the submissions.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndata_dir = '../input/commonlitreadabilityprize/'\ntrain_filepath = f'{data_dir}train.csv'\ntest_filepath = f'{data_dir}test.csv'\nunigrams_filepath = '../input/gwordlist/frequency-all.txt'\n\ntrain_data = pd.read_csv(train_filepath)\ntrain_data.standard_error = np.maximum(train_data.standard_error, 0.4)\n# train_data = train_data[((train_data['target'] != 0) & (train_data['standard_error'] != 0))]\ntrain_data, val_data, _, _ = train_test_split(\n    train_data, train_data['target'], test_size=0.2, random_state=18)\ntrain_data = train_data.round(1)\n#train_data.sort_values(by='target', inplace=True)\ntest_data = pd.read_csv(test_filepath)\ntest_data['target'] = 0\ntest_data['standard_error'] = 0\n\n# Convert datasets to list of dictionaries\ntrain_data = train_data.to_dict('records')\nval_data = val_data.to_dict('records')\ntest_data = test_data.to_dict('records')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:44:25.81011Z","iopub.execute_input":"2021-05-22T21:44:25.81038Z","iopub.status.idle":"2021-05-22T21:44:26.805528Z","shell.execute_reply.started":"2021-05-22T21:44:25.81035Z","shell.execute_reply":"2021-05-22T21:44:26.804737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need a external file with unigram frequencies: https://github.com/hackerb9/gwordlist/blob/master/frequency-alpha-gcide.txt","metadata":{}},{"cell_type":"code","source":"raw_unigrams = pd.read_csv(unigrams_filepath, sep='\\t')\nraw_unigrams.columns = ['gold_content', 'count', 'percent', 'cumulative']\nunigrams = pd.DataFrame(raw_unigrams.gold_content.str.split(' ', 1).tolist(),\n                                 columns = ['ranking','unigram'])\nunigrams['unigram'] = unigrams['unigram'].str.strip()\nunigrams['ranking'] = pd.to_numeric(unigrams[\"ranking\"])\nunigrams_dict = dict(zip(unigrams['unigram'], unigrams['ranking']))\nunigrams","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:44:26.807216Z","iopub.execute_input":"2021-05-22T21:44:26.807527Z","iopub.status.idle":"2021-05-22T21:45:10.385867Z","shell.execute_reply.started":"2021-05-22T21:44:26.807493Z","shell.execute_reply":"2021-05-22T21:45:10.385059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Oversampling\n\nTop 500 samples that are near the quadratic function that best fit target vs standard error.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nsubset = pd.DataFrame(train_data)\nsubset_1 = subset[((subset['target'] >= -0.5) & (subset['target'] < 0))]\nsubset_2 = subset[subset['target'] < -2.5]\nsubset_3 = subset[((subset['target'] >= -1.5) & (subset['target'] < -1.0))]\ntrain_data.extend(subset_1.to_dict('records'))\ntrain_data.extend(subset_2.to_dict('records'))\ntrain_data.extend(subset_2.to_dict('records'))\ntrain_data.extend(subset_3.to_dict('records'))\n\noversampled_data = pd.DataFrame(train_data)\noversampled_data['residuals'] = 0.489 + 0.0377 * oversampled_data['target'] + 0.0197 * oversampled_data['target'] * oversampled_data['target'] - oversampled_data['standard_error']\noversampled_data['residuals'] = oversampled_data['residuals'].abs()\noversampled_data.sort_values(by='residuals', inplace=True)\noversampled_data = oversampled_data.to_dict('records')\ntrain_data.extend(oversampled_data[:500])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:45:10.387399Z","iopub.execute_input":"2021-05-22T21:45:10.387741Z","iopub.status.idle":"2021-05-22T21:45:10.39401Z","shell.execute_reply.started":"2021-05-22T21:45:10.387706Z","shell.execute_reply":"2021-05-22T21:45:10.393105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Manual features","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")\n\ndef get_manual_features(text):\n    # Inspiration: https://arxiv.org/pdf/2103.04083v1.pdf\n    doc = nlp(text)\n    difficulty_embedding = []\n    embedding_size = 32\n    \n    num_words = len(doc)\n    num_sentences = len(list(doc.sents))\n    num_characters = len(text)\n    num_pronouns = 0\n    num_long_words = 0\n    num_letters = 0\n    words = {}\n\n    for index, token in enumerate(doc):\n        word_difficulty = unigrams_dict.get(token.text, None)\n        if word_difficulty:\n            difficulty_embedding.append(1 / word_difficulty)\n        if token.pos_ == 'PRON':\n            num_pronouns += 1\n        if len(token.text) > 6:\n            num_long_words += 1\n        if token.is_alpha:\n            num_letters += len(token.text)\n        words[token.text] = 1\n    \n    # Document level features\n    words_by_sentence = num_words / num_sentences\n    characters_by_word = num_characters / num_words\n    letters_by_word = num_letters / num_words\n    long_words_by_word = num_long_words / num_words\n    unique_words_by_word = len(words) / num_words\n    avg_difficulty = sum(difficulty_embedding) / len(difficulty_embedding)\n    \n    # Difficulty features\n    difficulty_embedding = list(set(difficulty_embedding))\n    difficulty_embedding.sort()\n    \n    manual_embedding = [\n        num_words, num_sentences, num_characters, num_pronouns,\n        num_long_words, num_letters, words_by_sentence, characters_by_word,\n        letters_by_word, long_words_by_word, unique_words_by_word,\n        unique_words_by_word, avg_difficulty\n    ]\n    manual_embedding.extend(difficulty_embedding[:embedding_size])\n    return manual_embedding\ntext_sample = '''\nMore people came to the bus stop just before 9am. Half an hour later they are all still waiting. Sam is worried. \"Maybe the bus broke down,\" he thinks. \"Maybe we won't go to town today. Maybe I won't get my new school uniform.\" At 9:45am some people give up and go home. Sam starts to cry. \"We will wait a bit longer,\" says his mother. Suddenly, they hear a noise. The bus is coming! The bus arrives at the stop at 10 o'clock. \"Get in! Get in!\" calls the driver. \"We are very late today!\" People get on the bus and sit down. The bus leaves the stop at 10:10am. \"What time is the return bus this afternoon?\" asks Sam's mother. \"The blue bus leaves town at 2:30pm,\" replies the driver. Sam thinks, \"We will get to town at 11 o'clock.\" \"How much time will we have in town before the return bus?\" wonders Sam.\n'''\nfeatures = get_manual_features(text_sample)\nfeatures","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:45:10.395437Z","iopub.execute_input":"2021-05-22T21:45:10.395799Z","iopub.status.idle":"2021-05-22T21:45:19.87952Z","shell.execute_reply.started":"2021-05-22T21:45:10.395764Z","shell.execute_reply":"2021-05-22T21:45:19.878689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, item in enumerate(train_data):\n    train_data[index]['manual_embedding'] = get_manual_features(train_data[index]['excerpt'])\n\nfor index, item in enumerate(val_data):\n    val_data[index]['manual_embedding'] = get_manual_features(val_data[index]['excerpt'])\n\nfor index, item in enumerate(test_data):\n    test_data[index]['manual_embedding'] = get_manual_features(test_data[index]['excerpt'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:45:19.880694Z","iopub.execute_input":"2021-05-22T21:45:19.881059Z","iopub.status.idle":"2021-05-22T21:46:43.572257Z","shell.execute_reply.started":"2021-05-22T21:45:19.881004Z","shell.execute_reply":"2021-05-22T21:46:43.571364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pooling Layers\n\nROBERTA produces useful sentence embeddings for the `<s>` token (equivalent to the [CLS] token in BERT). However, we can use (max and average) pooling to increase the semantics of embeddings.","metadata":{}},{"cell_type":"code","source":"# Mean Pooling - Take attention mask into account for correct averaging\n# Source: https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\ndef mean_pooling(token_embeddings, attention_mask):\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\n# Max Pooling - Take attention mask into account for correct max\n# Source: https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\ndef max_pooling(token_embeddings, attention_mask):\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    max_embeddings, max_indexes = torch.max(token_embeddings * input_mask_expanded, 1)\n    return max_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:46:43.57655Z","iopub.execute_input":"2021-05-22T21:46:43.578538Z","iopub.status.idle":"2021-05-22T21:46:43.587283Z","shell.execute_reply.started":"2021-05-22T21:46:43.578498Z","shell.execute_reply":"2021-05-22T21:46:43.586159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generator\n\nI started in Deep Learning with Tensorflow so I call Datasets as Data Generators","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModel\n\nMODEL_NAME = '../input/roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nroberta_model = AutoModel.from_pretrained(MODEL_NAME)\nroberta_model.to(device)\n\nclass CommonLitReadabilityDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        data = self.dataset[index]\n        \n        tokens = tokenizer(\n            data['excerpt'], return_tensors='pt', padding='max_length',\n            max_length=330\n        )\n        tokens.to(device)\n        outputs = roberta_model(**tokens)\n        token_embeddings = outputs.last_hidden_state\n\n        # Create the ROBERTA features for the model\n        s_token = token_embeddings[:, 0, :].detach().to('cpu').numpy()  # take <s> token (equiv. to [CLS])\n        max_embedding = max_pooling(token_embeddings, tokens['attention_mask'].to(device)).detach().to('cpu').numpy()\n        avg_embedding = mean_pooling(token_embeddings, tokens['attention_mask'].to(device)).detach().to('cpu').numpy()\n        token_embeddings = token_embeddings.detach().to('cpu').numpy()\n        \n        # Prepare the features for fine-tuning and additional features\n        response = {\n            'token_embeddings': token_embeddings,\n            's_token_embedding': s_token,\n            'max_embedding': max_embedding,\n            'avg_embedding': avg_embedding,\n            'manual_embedding': np.array([data['manual_embedding']]),\n            'target': np.array([data['target']]),\n            'standard_error': np.array([data['standard_error']])\n        }\n\n        return response","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:46:43.592563Z","iopub.execute_input":"2021-05-22T21:46:43.595103Z","iopub.status.idle":"2021-05-22T21:46:57.289751Z","shell.execute_reply.started":"2021-05-22T21:46:43.595065Z","shell.execute_reply":"2021-05-22T21:46:57.288888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the data loader for training and validation","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\n\n\ndef custom_collate(batch):\n    \"\"\"Get features and targets\"\"\"\n    # Features\n    token_embeddings = torch.from_numpy(np.array([item['token_embeddings'] for item in batch]))\n    token_embeddings = torch.squeeze(token_embeddings)\n    s_token_embedding = torch.from_numpy(np.array([item['s_token_embedding'] for item in batch]))\n    s_token_embedding = torch.squeeze(s_token_embedding)\n    max_embedding = torch.from_numpy(np.array([item['max_embedding'] for item in batch]))\n    max_embedding = torch.squeeze(max_embedding)\n    avg_embedding = torch.from_numpy(np.array([item['avg_embedding'] for item in batch]))\n    avg_embedding = torch.squeeze(avg_embedding)\n    manual_embedding = torch.from_numpy(np.array([item['manual_embedding'] for item in batch]))\n    manual_embedding = torch.squeeze(manual_embedding)\n    \n    # Targets\n    target = torch.FloatTensor([item['target'] for item in batch])\n    standard_error = torch.FloatTensor([item['standard_error'] for item in batch])\n\n    return {\n        'token_embeddings': token_embeddings,\n        's_token_embedding': s_token_embedding,\n        'max_embedding': max_embedding,\n        'avg_embedding': avg_embedding,\n        'manual_embedding': manual_embedding,\n        'target': target,\n        'standard_error': standard_error\n    }\n\nBATCH_SIZE = 24\n\nds_train = CommonLitReadabilityDataset(train_data)\ntraining_loader = DataLoader(\n    ds_train, batch_size=BATCH_SIZE, num_workers=0, collate_fn=custom_collate,\n    pin_memory=True, shuffle=True)\n\nds_val = CommonLitReadabilityDataset(val_data)\nvalidation_loader = DataLoader(\n    ds_val, batch_size=BATCH_SIZE, num_workers=0, collate_fn=custom_collate,\n    pin_memory=True)\n\nds_test = CommonLitReadabilityDataset(test_data)\ntest_loader = DataLoader(\n    ds_test, batch_size=BATCH_SIZE, num_workers=0, collate_fn=custom_collate,\n    pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:46:57.291302Z","iopub.execute_input":"2021-05-22T21:46:57.291625Z","iopub.status.idle":"2021-05-22T21:46:57.301403Z","shell.execute_reply.started":"2021-05-22T21:46:57.291586Z","shell.execute_reply":"2021-05-22T21:46:57.300625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import MSELoss\nfrom torch.distributions import Normal\nimport torch\n# Inspiration: https://github.com/huggingface/transformers/blob/c40c7e213bdd0479bdca69df0c500004a7294d39/src/transformers/models/roberta/modeling_roberta.py#L1384\n\nclass CommonLitReadabilityModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        EMBEDDING_SIZE = 768\n        HIDDEN_SIZE = 256\n        DROPOUT_RATE = 0.1\n        NUM_MANUAL_FEATURES = 45\n        # NUM_MANUAL_FEATURES = 0\n        \n        # Convolutional network to handle bigrams, trigrams, fourgrams, fivegrams and sixgrams\n        filter_sizes = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n        cnn_kernel_size = 128\n        self.conv1d_list = nn.ModuleList([\n            nn.Conv1d(\n                in_channels=EMBEDDING_SIZE,\n                out_channels=cnn_kernel_size,\n                kernel_size=filter_sizes[i]\n            )\n            for i in range(len(filter_sizes))\n        ])\n        \n        self.dense = nn.Linear(EMBEDDING_SIZE * 3 + cnn_kernel_size * len(filter_sizes) + NUM_MANUAL_FEATURES, HIDDEN_SIZE)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n        self.target_proj = nn.Linear(HIDDEN_SIZE, 1)\n        self.std_dev_proj = nn.Linear(HIDDEN_SIZE, 1)\n    \n    def forward(\n        self, token_embeddings, s_token_embedding, max_embedding, avg_embedding,\n        manual_embedding, target=None, standard_error=None):\n        # Permute the token_embedding to match the input shape for\n        # nn.Conv1d: (batch_size, embedding_size, max_sequence_length)\n        embedding_reshaped = token_embeddings.permute(0, 2, 1)\n        \n        # Apply CNN and ReLU. Output shape: (batch_size, num_filters[i], L_out)\n        x_conv_list = [F.relu(conv1d(embedding_reshaped)) for conv1d in self.conv1d_list]\n\n        # Max pooling. Output shape: (batch_size, num_filters[i], 1)\n        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n            for x_conv in x_conv_list]\n\n        # Concatenate x_pool_list to feed the fully connected layer.\n        # Output shape: (batch_size, sum(num_filters))\n        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n                         dim=1)\n\n        x = torch.cat((x_fc, s_token_embedding, max_embedding, avg_embedding, manual_embedding), dim=1)\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n\n        # Computing the logits\n        logits_target = self.target_proj(x)\n        logits_std_dev = self.std_dev_proj(x)\n        \n        # Computing the loss\n        loss = None\n        if target is not None:\n            \n            loss_fct = MSELoss()\n            loss_target = loss_fct(logits_target.view(-1, 1), target)\n            # loss = loss_target\n            loss_fct = MSELoss()\n            loss_std_dev = loss_fct(logits_std_dev.view(-1, 1), standard_error)\n            loss = loss_target + loss_std_dev * 0.65\n            \"\"\"\n            # https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n            # Not available for torch 1.7\n            loss_fct = GaussianNLLLoss()\n            loss = loss_fct(input=logits_target.view(-1, 1), target=target, var=standard_error ** 2)\n            \n            p = torch.distributions.Normal(logits_target.view(-1, 1), logits_std_dev.view(-1, 1))\n            q = torch.distributions.Normal(target, standard_error)\n            loss = torch.distributions.kl_divergence(p, q).mean()\n            \"\"\"\n        \n        return logits_target, loss","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:46:57.302588Z","iopub.execute_input":"2021-05-22T21:46:57.303076Z","iopub.status.idle":"2021-05-22T21:46:57.326838Z","shell.execute_reply.started":"2021-05-22T21:46:57.303042Z","shell.execute_reply":"2021-05-22T21:46:57.325988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training settings","metadata":{}},{"cell_type":"code","source":"from fastprogress import master_bar, progress_bar\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.nn as nn\nimport torch\n\nmodel = CommonLitReadabilityModel()\nmodel.to(device)\n\n# Karphaty LR = 3e-4\noptimizer = AdamW(\n    model.parameters(),\n    lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n)\n\nNUM_EPOCHS = 8\nmb = master_bar(range(NUM_EPOCHS))\n\n# Total number of training steps\nTOTAL_STEPS = len(training_loader) * NUM_EPOCHS\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = TOTAL_STEPS)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:46:57.328252Z","iopub.execute_input":"2021-05-22T21:46:57.328724Z","iopub.status.idle":"2021-05-22T21:47:03.272305Z","shell.execute_reply.started":"2021-05-22T21:46:57.328688Z","shell.execute_reply":"2021-05-22T21:47:03.271405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom copy import deepcopy\n\nbest_model_state = None\nbest_rmse = 1.0\n\nfor epoch in mb:\n    print(f'======================= Epoch {epoch + 1} / {NUM_EPOCHS} =========================')\n    \n    # Reset the total loss for this epoch in training phase\n    total_train_loss = 0\n    \n    # Training phase\n    model.train()\n    for batch in progress_bar(training_loader, parent=mb):\n        # Extract features\n        b_token_embeddings = batch['token_embeddings'].to(device).float()\n        b_s_token_embedding = batch['s_token_embedding'].to(device).float()\n        b_max_embedding = batch['max_embedding'].to(device).float()\n        b_avg_embedding = batch['avg_embedding'].to(device).float()\n        b_manual_embedding = batch['manual_embedding'].to(device).float()\n        b_target = batch['target'].to(device).float()\n        b_standard_error = batch['standard_error'].to(device).float()\n\n        # Reset the optimizer: Don't reuse info about the last batches\n        # It seems it is safer to zero_grad() the model instead of the optimizer\n        # Source: https://discuss.pytorch.org/t/model-zero-grad-or-optimizer-zero-grad/28426/6\n        model.zero_grad()\n        \n        # Calculate the forwards pass and the loss\n        _, loss = model(\n            b_token_embeddings, b_s_token_embedding, b_max_embedding,\n            b_avg_embedding, b_manual_embedding, b_target, b_standard_error)\n\n        # backward + optimize + schedule only if the model is in training phase\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        # Accumulate the training loss\n        total_train_loss += loss.item()\n    print(f\"Training loss: {total_train_loss / len(train_data)}\")\n    \n    # Reset the total loss for this epoch in validation phase.\n    total_val_loss = 0\n\n    # Tracking variables\n    val_labels = []\n    val_predictions = []\n    \n    # Validation phase\n    model.eval()\n    for batch in progress_bar(validation_loader, parent=mb):\n        # Extract features\n        b_token_embeddings = batch['token_embeddings'].to(device).float()\n        b_s_token_embedding = batch['s_token_embedding'].to(device).float()\n        b_max_embedding = batch['max_embedding'].to(device).float()\n        b_avg_embedding = batch['avg_embedding'].to(device).float()\n        b_manual_embedding = batch['manual_embedding'].to(device).float()\n        b_target = batch['target'].to(device).float()\n        b_standard_error = batch['standard_error'].to(device).float()\n        \n        # There is no need to compute the graph for the forward pass\n        # because we only need it for backprop (training)\n        with torch.no_grad():\n            # Calculate the forward pass and the loss\n            logits, loss = model(\n            b_token_embeddings, b_s_token_embedding, b_max_embedding,\n            b_avg_embedding, b_manual_embedding, b_target, b_standard_error)\n\n        # Accumulate the validation loss\n        total_val_loss += loss.item()\n        \n        # Move labels and logits to CPU\n        labels = b_target.to('cpu').numpy()\n        val_labels.extend(labels.tolist())\n        predictions = logits.to('cpu').numpy()\n        val_predictions.extend(predictions.tolist())\n    # Report validation metrics\n    val_rmse = mean_squared_error(val_labels, val_predictions, squared=False)\n    print(f'Validation loss = {total_val_loss / len(val_data)}')\n    print(f'Validation RSME = {val_rmse}')\n    \n    # Save best model\n    if val_rmse < best_rmse:\n        best_rmse = val_rmse\n        best_model_state = deepcopy(model.state_dict())","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:47:03.273591Z","iopub.execute_input":"2021-05-22T21:47:03.273908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error Analysis","metadata":{}},{"cell_type":"code","source":"best_model = CommonLitReadabilityModel()\nbest_model.load_state_dict(best_model_state)\nbest_model.to(device)\nbest_model.eval()\n\nval_predictions = []\nfor batch in validation_loader:\n    # Extract features\n    b_token_embeddings = batch['token_embeddings'].to(device).float()\n    b_s_token_embedding = batch['s_token_embedding'].to(device).float()\n    b_max_embedding = batch['max_embedding'].to(device).float()\n    b_avg_embedding = batch['avg_embedding'].to(device).float()\n    b_manual_embedding = batch['manual_embedding'].to(device).float()\n\n    # There is no need to compute the graph for the forward pass\n    # because we only need it for backprop (training)\n    with torch.no_grad():\n        # Calculate the forward pass and the loss\n        logits, _ = best_model(\n        b_token_embeddings, b_s_token_embedding, b_max_embedding,\n        b_avg_embedding, b_manual_embedding)\n\n    # Move logits to CPU\n    predictions = logits.to('cpu').numpy()\n    val_predictions.extend(predictions.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\n\nplt.scatter(val_labels, val_predictions, s=2, c='g', alpha=0.5)\nplt.plot(val_labels, val_labels)\nplt.xlabel(\"Target\")\nplt.ylabel(\"Predictions\")\nplt.title(f'R^2: {r2_score(val_labels, val_predictions)}')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map from readability scores to a \"class\"\n\ndef map_readability_scores(target_list):\n    new_targets = []\n    for target in target_list:\n        new_target = None\n        if target[0] < -2.5:\n            new_target = 0\n        elif target[0] >= -2.5 and target[0] < -2.0:\n            new_target = 1\n        elif target[0] >= -2.0 and target[0] < -1.5:\n            new_target = 2\n        elif target[0] >= -1.5 and target[0] < -1.0:\n            new_target = 3\n        elif target[0] >= -1.0 and target[0] < -0.5:\n            new_target = 4\n        elif target[0] >= -0.5 and target[0] < -0.0:\n            new_target = 5\n        elif target[0] >= 0.0 and target[0] < 0.5:\n            new_target = 6\n        elif target[0] >= 0.5 and target[0] < 1.0:\n            new_target = 7\n        elif target[0] >= 1.0 and target[0] < 1.5:\n            new_target = 8\n        elif target[0] >= 1.5 and target[0] < 2.0:\n            new_target = 9\n        elif target[0] >= 2.0 and target[0] < 2.5:\n            new_target = 10\n        elif target[0] >= 2.5:\n            new_target = 11\n        new_targets.append(new_target)\n    return new_targets\n\nlabel_names = ['-2.5', '-2.0', '-1.5', '-1.0', '-0.5', '0.0', '0.5', '1.0', '1.5', '2.0', '2.5', '3.0']\nclass_labels = map_readability_scores(val_labels)\nclass_predictions = map_readability_scores(val_predictions)\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(class_labels, class_predictions, labels=range(12))\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=label_names)\ndisp.plot(cmap=plt.cm.Blues)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating outputs\n\nPredict the outputs","metadata":{}},{"cell_type":"code","source":"\ntest_predictions = []\nbest_model.eval()\nfor batch in test_loader:\n    # Extract features\n    b_token_embeddings = batch['token_embeddings'].to(device).float()\n    b_s_token_embedding = batch['s_token_embedding'].to(device).float()\n    b_max_embedding = batch['max_embedding'].to(device).float()\n    b_avg_embedding = batch['avg_embedding'].to(device).float()\n    b_manual_embedding = batch['manual_embedding'].to(device).float()\n\n    # There is no need to compute the graph for the forward pass\n    # because we only need it for backprop (training)\n    with torch.no_grad():\n        # Calculate the forward pass and the loss\n        logits, _ = best_model(\n        b_token_embeddings, b_s_token_embedding, b_max_embedding,\n        b_avg_embedding, b_manual_embedding)\n\n    # Move logits to CPU\n    predictions = logits.to('cpu').numpy()\n    test_predictions.extend(predictions.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a pandas dataframe and save it as csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = np.array([test_predictions])[0]\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}