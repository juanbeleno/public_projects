{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of readability score using text\n",
    "\n",
    "We verify that GPU is available for training using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 13 21:51:00 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P8    N/A /  N/A |     37MiB /  2048MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Mar_21_19:24:09_Pacific_Daylight_Time_2021\n",
      "Cuda compilation tools, release 11.3, V11.3.58\n",
      "Build cuda_11.3.r11.3/compiler.29745058_0\n",
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# Verify CUDA version is 11.0+\n",
    "!nvcc -V\n",
    "\n",
    "# Verify pytorch is using the GPU\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, we need to load all the datasets that we are going to use to train the model and generate the submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# data_dir = '../input/commonlitreadabilityprize/'\n",
    "data_dir = 'inputs/'\n",
    "train_filepath = f'{data_dir}train.csv'\n",
    "test_filepath = f'{data_dir}test.csv'\n",
    "output_filepath = 'outputs/submission.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_filepath)\n",
    "train_data, val_data, _, _ = train_test_split(\n",
    "    train_data, train_data['target'], test_size=0.2, random_state=42)\n",
    "test_data = pd.read_csv(test_filepath)\n",
    "\n",
    "# Convert datasets to list of dictionaries\n",
    "train_data = train_data.to_dict('records')\n",
    "val_data = val_data.to_dict('records')\n",
    "test_data = test_data.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we build a dictionary with unigram frequencies from a external source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65532</th>\n",
       "      <td>65533</td>\n",
       "      <td>nitrifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65533</th>\n",
       "      <td>65534</td>\n",
       "      <td>fordo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65534</th>\n",
       "      <td>65535</td>\n",
       "      <td>elusory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65535</th>\n",
       "      <td>65536</td>\n",
       "      <td>foamless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65536</th>\n",
       "      <td>65537</td>\n",
       "      <td>exarticulation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65537 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ranking         unigram\n",
       "0            1             the\n",
       "1            2              of\n",
       "2            3             and\n",
       "3            4              to\n",
       "4            5              in\n",
       "...        ...             ...\n",
       "65532    65533       nitrifier\n",
       "65533    65534           fordo\n",
       "65534    65535         elusory\n",
       "65535    65536        foamless\n",
       "65536    65537  exarticulation\n",
       "\n",
       "[65537 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm using the ngrams available in https://github.com/hackerb9/gwordlist\n",
    "unigram_url = 'https://raw.githubusercontent.com/hackerb9/gwordlist/master/frequency-alpha-gcide.txt'\n",
    "raw_unigrams = pd.read_csv(unigram_url, sep='\\t')\n",
    "raw_unigrams.columns = ['gold_content', 'count', 'percent', 'cumulative']\n",
    "unigrams = pd.DataFrame(raw_unigrams.gold_content.str.split(' ', 1).tolist(),\n",
    "                                 columns = ['ranking','unigram'])\n",
    "unigrams['unigram'] = unigrams['unigram'].str.strip()\n",
    "unigrams['ranking'] = pd.to_numeric(unigrams[\"ranking\"])\n",
    "unigrams_dict = dict(zip(unigrams['unigram'], unigrams['ranking']))\n",
    "unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "References:\n",
    "\n",
    "https://www.kaggle.com/abhilashreddyy/a-deeper-eda-on-pos-tags-topic-modelling-more\n",
    "\n",
    "https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "ROBERTA produces useful sentence embeddings for the `<s>` token (equivalent to the [CLS] token in BERT). However, we can use (max and average) pooling to increase the semantics of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "# Source: https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Max Pooling - Take attention mask into account for correct max\n",
    "# Source: https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\n",
    "def max_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    max_embeddings, max_indexes = torch.max(token_embeddings * input_mask_expanded, 1)\n",
    "    return max_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "I started in Deep Learning with Tensorflow so I call Datasets as Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "MODEL_NAME = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "roberta_model = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "roberta_model.to(device)\n",
    "\n",
    "class CommonLitReadabilityDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        \n",
    "        tokens = tokenizer(data['excerpt'], return_tensors='pt')\n",
    "        tokens.to(device)\n",
    "        outputs = roberta_model(**tokens)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Create the ROBERTA features for the model\n",
    "        s_token = token_embeddings[:, 0, :].detach().to('cpu').numpy()  # take <s> token (equiv. to [CLS])\n",
    "        max_embedding = mean_pooling(token_embeddings, tokens['attention_mask'].to(device)).detach().to('cpu').numpy()\n",
    "        avg_embedding = max_pooling(token_embeddings, tokens['attention_mask'].to(device)).detach().to('cpu').numpy()\n",
    "        token_embeddings = token_embeddings.detach().to('cpu').numpy()\n",
    "        \n",
    "        # Prepare the features for fine-tuning and additional features\n",
    "        response = {\n",
    "            'token_embeddings': token_embeddings,\n",
    "            's_token_embedding': s_token,\n",
    "            'max_embedding': max_embedding,\n",
    "            'avg_embedding': avg_embedding,\n",
    "            'target': np.array([data['target']]),\n",
    "            'standard_error': np.array([data['standard_error']])\n",
    "        }\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data loader for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"Get features and targets\"\"\"\n",
    "    # Features\n",
    "    s_token_embedding = torch.from_numpy(np.array([item['s_token_embedding'] for item in batch]))\n",
    "    s_token_embedding = torch.squeeze(s_token_embedding)\n",
    "    max_embedding = torch.from_numpy(np.array([item['max_embedding'] for item in batch]))\n",
    "    max_embedding = torch.squeeze(max_embedding)\n",
    "    avg_embedding = torch.from_numpy(np.array([item['avg_embedding'] for item in batch]))\n",
    "    avg_embedding = torch.squeeze(avg_embedding)\n",
    "    \n",
    "    # Targets\n",
    "    target = torch.FloatTensor([item['target'] for item in batch])\n",
    "    standard_error = torch.FloatTensor([item['standard_error'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'token_embeddings': torch.from_numpy(np.array([])),\n",
    "        's_token_embedding': s_token_embedding,\n",
    "        'max_embedding': max_embedding,\n",
    "        'avg_embedding': avg_embedding,\n",
    "        'target': target,\n",
    "        'standard_error': standard_error\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "ds_train = CommonLitReadabilityDataset(train_data)\n",
    "training_loader = DataLoader(\n",
    "    ds_train, batch_size=BATCH_SIZE, num_workers=0, collate_fn=custom_collate,\n",
    "    pin_memory=True, shuffle=True)\n",
    "\n",
    "ds_val = CommonLitReadabilityDataset(val_data)\n",
    "validation_loader = DataLoader(\n",
    "    ds_val, batch_size=BATCH_SIZE, num_workers=0, collate_fn=custom_collate,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import torch\n",
    "# Inspiration: https://github.com/huggingface/transformers/blob/c40c7e213bdd0479bdca69df0c500004a7294d39/src/transformers/models/roberta/modeling_roberta.py#L1384\n",
    "\n",
    "class CommonLitReadabilityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        EMBEDDING_SIZE = 768\n",
    "        HIDDEN_SIZE = 32\n",
    "        DROPOUT_RATE = 0.1\n",
    "        \n",
    "        self.dense = nn.Linear(EMBEDDING_SIZE * 3, HIDDEN_SIZE)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.target_proj = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.std_dev_proj = nn.Linear(HIDDEN_SIZE, 1)\n",
    "    \n",
    "    def forward(self, token_embeddings, s_token_embedding, max_embedding, avg_embedding, target=None, standard_error=None):\n",
    "        x = torch.cat((s_token_embedding, max_embedding, avg_embedding), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Computing the logits\n",
    "        logits_target = self.target_proj(x)\n",
    "        logits_std_dev = self.std_dev_proj(x)\n",
    "        \n",
    "        # Computing the loss\n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss_fct = MSELoss()\n",
    "            loss_target = loss_fct(logits_target.view(-1, 1), target)\n",
    "            loss = loss_target\n",
    "            # loss_fct = MSELoss()\n",
    "            # loss_std_dev = loss_fct(logits_std_dev.view(-1, 1), standard_error)\n",
    "            # loss = loss_target + loss_std_dev\n",
    "        \n",
    "        return logits_target, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model = CommonLitReadabilityModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 3e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "mb = master_bar(range(NUM_EPOCHS))\n",
    "\n",
    "# Total number of training steps\n",
    "TOTAL_STEPS = len(training_loader) * NUM_EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for epoch in mb:\n",
    "    print(f'======================= Epoch {epoch + 1} / {NUM_EPOCHS} =========================')\n",
    "    \n",
    "    # Reset the total loss for this epoch in training phase\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for batch in progress_bar(training_loader, parent=mb):\n",
    "        # Extract features\n",
    "        b_token_embeddings = batch['token_embeddings'].to(device).float()\n",
    "        b_s_token_embedding = batch['s_token_embedding'].to(device).float()\n",
    "        b_max_embedding = batch['max_embedding'].to(device).float()\n",
    "        b_avg_embedding = batch['avg_embedding'].to(device).float()\n",
    "        b_target = batch['target'].to(device).float()\n",
    "        b_standard_error = batch['standard_error'].to(device).float()\n",
    "\n",
    "        # Reset the optimizer: Don't reuse info about the last batches\n",
    "        # It seems it is safer to zero_grad() the model instead of the optimizer\n",
    "        # Source: https://discuss.pytorch.org/t/model-zero-grad-or-optimizer-zero-grad/28426/6\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Calculate the forwards pass and the loss\n",
    "        _, loss = model(\n",
    "            b_token_embeddings, b_s_token_embedding, b_max_embedding,\n",
    "            b_avg_embedding, b_target, b_standard_error)\n",
    "\n",
    "        # backward + optimize + schedule only if the model is in training phase\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Accumulate the training loss\n",
    "        total_train_loss += loss.item()\n",
    "    print(f\"Training loss: {total_train_loss / len(train_data)}\")\n",
    "    \n",
    "    # Reset the total loss for this epoch in validation phase.\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Tracking variables\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    for batch in progress_bar(validation_loader, parent=mb):\n",
    "        # Extract features\n",
    "        b_token_embeddings = batch['token_embeddings'].to(device).float()\n",
    "        b_s_token_embedding = batch['s_token_embedding'].to(device).float()\n",
    "        b_max_embedding = batch['max_embedding'].to(device).float()\n",
    "        b_avg_embedding = batch['avg_embedding'].to(device).float()\n",
    "        b_target = batch['target'].to(device).float()\n",
    "        b_standard_error = batch['standard_error'].to(device).float()\n",
    "        \n",
    "        # There is no need to compute the graph for the forward pass\n",
    "        # because we only need it for backprop (training)\n",
    "        with torch.no_grad():\n",
    "            # Calculate the forward pass and the loss\n",
    "            logits, loss = model(\n",
    "            b_token_embeddings, b_s_token_embedding, b_max_embedding,\n",
    "            b_avg_embedding, b_target, b_standard_error)\n",
    "\n",
    "        # Accumulate the validation loss\n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "        # Move labels and logits to CPU\n",
    "        labels = b_target.to('cpu').numpy()\n",
    "        val_labels.extend(labels.tolist())\n",
    "        predictions = logits.to('cpu').numpy()\n",
    "        val_predictions.extend(predictions.tolist())\n",
    "    # Report validation metrics\n",
    "    print(f'Validation loss = {total_val_loss / len(val_data)}')\n",
    "    print(f'Validation RSME = {mean_squared_error(val_labels, val_predictions, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "plt.scatter(val_labels, val_predictions, s=2, c='g', alpha=0.5)\n",
    "plt.plot(val_labels, val_labels)\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(f'R^2: {r2_score(val_labels, val_predictions)}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
